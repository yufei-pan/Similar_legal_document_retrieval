{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)\n",
    "nltk.download('stopwords', quiet = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    #expand contractions\n",
    "    X['Text'] = X['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "    #join back words\n",
    "    X['Text'] = [' '.join(map(str, l)) for l in X['Text']]\n",
    "    #lowercase the reviews\n",
    "    X['Text'] = X['Text'].str.lower()\n",
    "    #remove html and url form reviews\n",
    "    X['Text'] = X['Text'].str.replace(r'\\s*https?://\\S+(\\s+|$)', '', regex=True).str.strip()\n",
    "    #remove non-alphabetical characters\n",
    "    X['Text'] = X['Text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "    #remove extra spaces\n",
    "    X['Text'] = X['Text'].replace(r'\\s+', ' ', regex=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(X):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    X['Text'] = X['Text'].apply(tokenize.word_tokenize)\n",
    "    return X['Text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(X):\n",
    "    tfidf=TfidfVectorizer(min_df = 50, max_df=0.95, ngram_range = (1,3), max_features=1500, norm='l2')\n",
    "    X_data = tfidf.fit_transform(X)\n",
    "    return X_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file and convert to dataframe\n",
    "df = pd.read_csv('csv/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish base line performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess case_text using preocess_text function\n",
    "data = df[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = [' '.join(map(str, l)) for l in X['Text']]\n",
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].str.lower()\n",
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].str.replace(r'\\s*https?://\\S+(\\s+|$)', '', regex=True).str.strip()\n",
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/1146609196.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].replace(r'\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "case_text = preprocess_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/311661285.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Text'] = X['Text'].apply(tokenize.word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "case_text = tokenize_and_remove_stopwords(case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_text = case_text.apply(nltk.tag.pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_text = case_text.apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "case_text = case_text.apply(lambda x: [lemma.lemmatize(word, tag) for word, tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_text = [' '.join(map(str, l)) for l in case_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text using tfidf vectorizer\n",
    "X_data = vectorize_tfidf(case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 434)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from gensim) (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(case_text, vector_size=300, window=5, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_embeddings(X):\n",
    "    embeddings = []\n",
    "    for sentence in X:\n",
    "        sentence_embedding = np.zeros(300)\n",
    "        for word in sentence:\n",
    "            if word in w2v_model.wv:\n",
    "                sentence_embedding += w2v_model.wv[word]\n",
    "        embeddings.append(sentence_embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = create_word2vec_embeddings(case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5  4  8  6  7  3 44 58  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  5,  4,  8,  6,  7,  3, 44, 58,  1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find most similar cases using cosine similarity and tfidf vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_cases(X, case_id, n):\n",
    "    pairwise_similarities=np.dot(X,X.T)\n",
    "    cosine_similarities = cosine_similarity(X[case_id].reshape(1,-1), X).flatten()\n",
    "\n",
    "    highest_score_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "    print(highest_score_indices)\n",
    "\n",
    "    return highest_score_indices\n",
    "\n",
    "#find most similar cases using cosine similarity and word2vec vectors\n",
    "\n",
    "find_similar_cases(X_data, 0, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "       for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      \t\tyield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def process_doc2vec_similarity(X,case_id):\n",
    "\n",
    "\tdocuments = list(tagged_document(X))\n",
    "\t\t\n",
    "\tmodel = gensim.models.doc2vec.Doc2Vec(documents, vector_size=300, window=11, min_count=10, epochs=30)\n",
    "\n",
    "\n",
    "\t# Only handle words that appear in the doc2vec pretrained vectors. enwiki_ebow model contains 669549 vocabulary size.\n",
    "\t# tokens = list(filter(lambda x: x in model.wv.vocab.keys(), documents[case_id].words))\n",
    "\n",
    "\tbase_vector = model.infer_vector(documents[case_id].words.split())\n",
    "\n",
    "\tvectors = []\n",
    "\tfor i, document in enumerate(documents):\n",
    "\n",
    "\t\t# tokens = list(filter(lambda x: x in model.wv.vocab.keys(), document))\n",
    "\t\tvector = model.infer_vector(document.words.split())\n",
    "\t\tvectors.append(vector)\n",
    "\n",
    "\t\t# print(\"making vector at index:\", i)\n",
    "\n",
    "\tscores = cosine_similarity([base_vector], vectors).flatten()\n",
    "\n",
    "\t# top 10 highest scores\n",
    "\thighest_score_indices = scores.argsort()[-10:][::-1]\n",
    "\n",
    "\t\n",
    "\tprint(highest_score_indices)\n",
    "\n",
    "\n",
    "\n",
    "\thighest_score = 0\n",
    "\thighest_score_index = 0\n",
    "\tfor i, score in enumerate(scores):\n",
    "\t\tif highest_score < score:\n",
    "\t\t\thighest_score = score\n",
    "\t\t\thighest_score_index = i\n",
    "\n",
    "\tmost_similar_document = documents[highest_score_index]\n",
    "\tprint(\"Most similar document by Doc2vec with the score:\", most_similar_document, highest_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 37  9 40 36 71 11 38 12 46]\n",
      "Most similar document by Doc2vec with the score: TaggedDocument<united state supreme court hunt v palao argue decide january motion make bring record case decide territorial court appeal florida previously admission florida state motion follow mr westcott behalf john hunt submit court certify copy record opinion say court appeal say judgment say case suggest court say court appeal defunct admission territory florida state th march last record paper say court appeal record aforesaid say case place act general assembly say state custody keep clerk supreme court say state also say case case federal jurisdiction move court allow writ error remove say record judgment court direction clerk court direct judge say supreme court say state clerk aforesaid custody say record aforesaid order say record judgment may certify court return say writ error make say clerk say supreme court say state mr chief justice taney deliver opinion court motion make process court bring revision record proceeding late territorial court appeal florida case hunt v lessee palao judgment render favor latter february term since florida cease territory become state law pass state directing record paper mention territorial court place custody clerk supreme court state law record case question possession safe keep congress make special provision case kind appellate power court exercise must exercise manner prescribed general law congress upon subject act writ error territorial court appeal prosecute accord provision regulation twenty fifth section judiciary act assume case question one subject revision court accord act congress yet appellate power must exercise manner prescribe law act writ error must direct court hold proceeding part record exercise judicial power court render judgment case u long existence proceeding possession court authorize exercise judicial power possession officer another court merely purpose safe keep law florida place record custody state court clerk subject control court manner regard indeed place custody court would remove difficulty law state could make record court authorize proceeding upon territorial court appeal court united state control record therefore belong general government state authority rest congress declare tribunal record proceeding shall transfer judgment shall carry execution review upon appeal writ error suggest writ error may issue fourteenth section act person actual custody record upon ground writ necessary exercise appellate power court language section would justify construction record proceeding bring writ error either supreme court state clerk judgment territorial court find erroneous reverse still tribunal authorized send mandate proceed case carry execution judgment court may pronounce certainly could send supreme court state judgment record power execute judgment give territorial court neither reason could send mandate district court unite state unless authorize law congress would useless vain court issue writ error bring record proceed judgment upon law stand mean process authorized judgment could execute think therefore judgment decree render late territorial court review writ error appeal unless provision subject shall make congress consequently motion case must refuse, [0]> 0.99608684\n"
     ]
    }
   ],
   "source": [
    "process_doc2vec_similarity(case_text, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 309 459 366  19 149  22 162 277  52]\n",
      "Most similar document by Doc2vec with the score: TaggedDocument<united, [0]> 1.0\n"
     ]
    }
   ],
   "source": [
    "process_doc2vec_similarity(case_text,309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def doc2vec_vectors(X):\n",
    "   documents = list(tagged_document(X[0].split()))\n",
    "   model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=80)\n",
    "   model.build_vocab(documents)\n",
    "   model.train(documents, total_examples=model.corpus_count, epochs=80)\n",
    "\n",
    "   # model = gensim.models.doc2vec.Doc2Vec(documents, vector_size=300, window=11, min_count=10, epochs=30)\n",
    "   return np.array([model.infer_vector(doc.split()) for doc in X]),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec,model = doc2vec_vectors(case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/vx_6_8q177s3k40k665prfww0000gn/T/ipykernel_98535/3754858182.py:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  model.docvecs.most_similar(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(455, 0.996326208114624),\n",
       " (185, 0.9962396621704102),\n",
       " (433, 0.9962393641471863),\n",
       " (225, 0.9962005615234375),\n",
       " (437, 0.9961450695991516),\n",
       " (385, 0.9961084127426147),\n",
       " (379, 0.99603670835495),\n",
       " (330, 0.9960185289382935),\n",
       " (414, 0.9960132837295532),\n",
       " (200, 0.9960082769393921)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/protobuf/pyext/_message.cpython-310-darwin.so, 0x0002): symbol not found in flat namespace (__ZNK6google8protobuf10TextFormat21FastFieldValuePrinter19PrintMessageContentERKNS0_7MessageEiibPNS1_17BaseTextGeneratorE)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/context.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m _b\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m (\u001b[39mlambda\u001b[39;00m x:x) \u001b[39mor\u001b[39;00m (\u001b[39mlambda\u001b[39;00m x:x\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor \u001b[39mas\u001b[39;00m _descriptor\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m message \u001b[39mas\u001b[39;00m _message\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m reflection \u001b[39mas\u001b[39;00m _reflection\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/protobuf/descriptor.py:47\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[39mimport\u001b[39;00m \u001b[39mbinascii\u001b[39;00m\n\u001b[1;32m     46\u001b[0m   \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyext\u001b[39;00m \u001b[39mimport\u001b[39;00m _message\n\u001b[1;32m     48\u001b[0m   _USE_C_DESCRIPTORS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mError\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/protobuf/pyext/_message.cpython-310-darwin.so, 0x0002): symbol not found in flat namespace (__ZNK6google8protobuf10TextFormat21FastFieldValuePrinter19PrintMessageContentERKNS0_7MessageEiibPNS1_17BaseTextGeneratorE)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#find the max sequence length of word2vec vectors\\\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(case_text)\n",
    "tokenized_documents=tokenizer.texts_to_sequences(case_text)\n",
    "tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=5000,padding='post')\n",
    "vocab_size=len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = create_word2vec_embeddings(case_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "! conda install -c conda-forge sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: nltk in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: tqdm in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: scipy in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: torchvision in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: numpy in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (1.23.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: requests in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/nimishamittal/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: click in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/nimishamittal/.local/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nimishamittal/miniforge3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39m pip install sentence-transformers\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mbert-base-nli-mean-tokens\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimishamittal/Documents/USC/CSCI544_ANLP/Project/process_file.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(case_text)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings = model.encode(case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similar = cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[:]\n",
    ")\n",
    "ind = np.argsort(similar[0])[::-1][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
