{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# ! conda install -c conda-forge sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ! pip install transformers -q\n",
    "# ! pip install simpletransformers wandb pytorch-lightning\n",
    "# ! pip install -U transformers torch sentencepiece\n",
    "# ! pip install -U summa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textbf{All Pre-processing Functions on Top}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_pickle(path,compression = 'zstd')\n",
    "    df = df.dropna(how=\"any\")\n",
    "    return df\n",
    "\n",
    "def tokenize_and_remove_stopwords(X):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return X['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "\n",
    "    def remove_space(match_obj):\n",
    "        if match_obj.group() is not None:\n",
    "            return match_obj.group().replace(' ','|')\n",
    "\n",
    "    df['Text'] = df['Text'].str.replace(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?', 'URL', regex=True)\n",
    "\n",
    "\n",
    "    # remove '|' so we can use it as a unique space seperator for cases\n",
    "    df['Text'] = df['Text'].str.replace(r'|', '', regex=True)\n",
    "\n",
    "\n",
    "    df['Text'] = df['Text'].str.replace(r'U\\. S\\. C\\.', 'U.S.C.', regex=True)\n",
    "    df['Text'] = df['Text'].str.replace(r'U\\. S\\.', 'U.S.', regex=True)\n",
    "    df['Text'] = df['Text'].str.replace(r'No\\. ', 'No.', regex=True)\n",
    "    r\"F\\.( \\d+-?\\w*,?)+( \\([A-Z]*\\d* ?\\d*\\),?)*\"\n",
    "    # df['Text'] = df['Text'].str.replace(r\"F\\.( \\d+-?\\w*,?)+( \\([A-Z]*\\d* ?\\d*\\),?)*\", remove_space, regex=True)\n",
    "    text = []\n",
    "    for element in df['Text']:\n",
    "        element = re.sub(r\"\\bF\\.( \\d+-?\\w*,?)+( \\([A-Z]*\\d* ?\\d*\\),?)*\", remove_space, element)\n",
    "        element = re.sub(r\"\\b(\\d+ )?U\\.S\\.C\\. [A-Z]+\\d*\", remove_space, element)\n",
    "        element = re.sub(r\"\\b(\\d+ )?U\\.S\\.(( |-)\\(?\\d+,?\\)?)+\", remove_space, element)\n",
    "        element = re.sub(r\"\\bArt\\. [A-Z]+,?( [A-Z]\\d+)?\", remove_space, element)\n",
    "        element = re.sub(r\"\\b\\d{4} [A-Z]+ \\d+,?( \\*\\d*)?\", remove_space, element)\n",
    "        element = re.sub(r\"\\bn\\.( \\d+)+( \\(.+?\\))?\", remove_space, element)\n",
    "        element = re.sub(r\"\\bPp\\. [0-9\\-]+\", remove_space, element)\n",
    "        # element = re.sub(r\"\\b([A-Z]+[a-z]*[A-Z\\.']+,? )+v\\.( [A-Z]+[a-z]*[A-Z\\.']+,?)+ ?\", remove_space, element)\n",
    "        element = re.sub(r\"\\b\\d+ U\\.S.[_,\\- ]*\\(\\d+\\)\", remove_space, element)\n",
    "        text.append(element)\n",
    "    df['Text'] = text\n",
    "    # Disabled as legal cases need punctuations to work\n",
    "    # # remove non-alphabetical\n",
    "    # df['Text'] = df['Text'].str.replace('[^a-zA-Z0-9\\'\\\".!()]', ' ', regex=True).str.strip()\n",
    "\n",
    "\n",
    "    # remove extra spaces\n",
    "    df['Text'] = df['Text'].str.replace(' +', ' ', regex=True).str.strip()\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_VS_data(df):\n",
    "\n",
    "    #assert tokens column is present\n",
    "    assert 'tokens' in df.columns\n",
    "\n",
    "    new_tokens = []\n",
    "    for case in df['tokens']:\n",
    "        new_token = []\n",
    "        case_found = False\n",
    "        for tokenIdx in range(len(case)):\n",
    "            new_token.append(case[tokenIdx])\n",
    "            # Handling cases\n",
    "            #United States v. Rostenkowski, 59 F. 3d 1291, 1297 (CADC 1995).\n",
    "            #United States Supreme Court AARON J. SCHOCK v. UNITED STATES(2019) No. 18-406\n",
    "            if case[tokenIdx] == 'v.':\n",
    "                case_found = True\n",
    "            elif case_found and (case[tokenIdx].startswith('No.') or case[tokenIdx][0].islower() or case[tokenIdx][0].isnumeric() or case[tokenIdx-1].lower().startswith('al.')):\n",
    "                # we need to deal with this\n",
    "                last_word = new_token.pop()\n",
    "                castStr = ''\n",
    "                while len(new_token) > 0 and (new_token[-1] == 'v.' or (new_token[-1].lower() != 'see' and new_token[-1][0].isupper())) :\n",
    "                    castStr =  new_token.pop() + '|' + castStr\n",
    "                new_token.append(castStr[:-1])\n",
    "                case_found = False\n",
    "                new_token.append(last_word)\n",
    "        new_tokens.append(new_token)\n",
    "    df['tokens'] = new_tokens\n",
    "\n",
    "    # change all tokens to lower case\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [item.lower() for item in x])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "path = 'data/all_year.pkl.zst'\n",
    "df = read_data(path)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "#split the processed text into tokens for further processing with pipes | as the combinator\n",
    "df['tokens'] = df['Text'].str.split() \n",
    "df['tokens'] = process_VS_data(df)['tokens']\n",
    "df['tokens'] = tokenize_and_remove_stopwords(df)\n",
    "df['new_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "#apply nlp to the new text\n",
    "df['nlp'] = df['new_text'].apply(lambda x: nlp(x))\n",
    "#for each nlp object, get lemma and pos\n",
    "df['lemma'] = df['nlp'].apply(lambda x: [token.lemma_ for token in x])\n",
    "# remove punctuations from lemma\n",
    "df['lemma'] = df['lemma'].apply(lambda x: [token for token in x if token not in string.punctuation])\n",
    "# create a new column and convert lemma to string\n",
    "df['final_text'] = df['lemma'].apply(lambda x: ' '.join(x))\n",
    "# remove '\\'s' from final_text\n",
    "df['final_text'] = df['final_text'].str.replace(r'\\'s', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textbf{Text Summarization Functions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from summa import summarizer\n",
    "\n",
    "def summarize_pipeline(df):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    df[\"summary_pipeline\"] = df[\"final_text\"].apply(lambda x: summarizer(x, truncation=True, max_length=1024, min_length=300, do_sample=False)[0]['summary_text'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_summa(df):\n",
    "    df[\"summary_summa\"] = df[\"final_text\"].apply(lambda x: summarizer.summarize(x, ratio=0.02))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Your max_length is set to 1024, but you input_length is only 775. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=387)\n"
     ]
    }
   ],
   "source": [
    "# applied pipeline method to summarize text\n",
    "df = summarize_pipeline(df)\n",
    "\n",
    "# applied summa method to summarize text\n",
    "df = summarize_summa(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textbf{Vectorization Functions}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "       for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      \t\tyield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def process_doc2vec_similarity(X,case_id):\n",
    "\n",
    "\tdocuments = list(tagged_document(X))\n",
    "\t\t\n",
    "\tmodel = gensim.models.doc2vec.Doc2Vec(documents, vector_size=300, window=11, min_count=10, epochs=30)\n",
    "\n",
    "\n",
    "\t# Only handle words that appear in the doc2vec pretrained vectors. enwiki_ebow model contains 669549 vocabulary size.\n",
    "\t# tokens = list(filter(lambda x: x in model.wv.vocab.keys(), documents[case_id].words))\n",
    "\n",
    "\tbase_vector = model.infer_vector(documents[case_id].words.split())\n",
    "\n",
    "\tvectors = []\n",
    "\tfor i, document in enumerate(documents):\n",
    "\n",
    "\t\t# tokens = list(filter(lambda x: x in model.wv.vocab.keys(), document))\n",
    "\t\tvector = model.infer_vector(document.words.split())\n",
    "\t\tvectors.append(vector)\n",
    "\n",
    "\t\t# print(\"making vector at index:\", i)\n",
    "\n",
    "\tscores = cosine_similarity([base_vector], vectors).flatten()\n",
    "\n",
    "\t# top 10 highest scores\n",
    "\thighest_score_indices = scores.argsort()[-5:][::-1]\n",
    "\n",
    "\t\n",
    "\tprint(highest_score_indices)\n",
    "\n",
    "\thighest_score = 0\n",
    "\thighest_score_index = 0\n",
    "\tfor i, score in enumerate(scores):\n",
    "\t\tif highest_score < score and i != case_id:\n",
    "\t\t\thighest_score = score\n",
    "\t\t\thighest_score_index = i\n",
    "\n",
    "\tmost_similar_document = documents[highest_score_index]\n",
    "\tprint(\"Most similar document by Doc2vec with the score:\", highest_score, highest_score_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Text\n",
      "[1 0 9 3 7]\n",
      "Most similar document by Doc2vec with the score: 0.99545455 0\n",
      "Summary Pipeline\n",
      "[ 4 13  1 12  7]\n",
      "Most similar document by Doc2vec with the score: 0.99699116 4\n",
      "Summary Summa\n",
      "[ 6  2  5  1 12]\n",
      "Most similar document by Doc2vec with the score: 0.9946988 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Text\")\n",
    "process_doc2vec_similarity(df['final_text'], 1)\n",
    "\n",
    "print(\"Summary Pipeline\")\n",
    "process_doc2vec_similarity(df['summary_pipeline'], 1)\n",
    "\n",
    "print(\"Summary Summa\")\n",
    "process_doc2vec_similarity(df['summary_summa'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(X):\n",
    "    tfidf=TfidfVectorizer(min_df = 0.01, max_df=0.95, ngram_range = (1,3), max_features=1500, norm='l2')\n",
    "    X_data = tfidf.fit_transform(X)\n",
    "    return X_data.toarray()\n",
    "\n",
    "def process_tfidf_similarity(X,case_id):\n",
    "    X_data = vectorize_tfidf(X)\n",
    "    cosine_similarities = linear_kernel(X_data[case_id:case_id+1], X_data).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "    print(related_docs_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Text\n",
      "[ 1 14 10 12]\n",
      "Summary Pipeline\n",
      "[ 1 14 10  5]\n",
      "Summary Summa\n",
      "[ 1 14 12  5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Text\")\n",
    "process_tfidf_similarity(df['final_text'], 1)\n",
    "\n",
    "print(\"Summary Pipeline\")\n",
    "process_tfidf_similarity(df['summary_pipeline'], 1)\n",
    "\n",
    "print(\"Summary Summa\")\n",
    "process_tfidf_similarity(df['summary_summa'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_similarity(X,case_id):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sentences = X\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    query = sentences[case_id]\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "    number_top_matches = 5\n",
    "    similar_sentences = cosine_similarity([query_embedding], sentence_embeddings)[0].argsort()[-number_top_matches:][::-1]\n",
    "    print(similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Text\n",
      "[ 1  7  9 13  6]\n",
      "Summary Pipeline\n",
      "[ 1  7  9 13  3]\n",
      "Summary Summa\n",
      "[ 1 14  0 12  2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Text\")\n",
    "bert_similarity(df['final_text'], 1)\n",
    "\n",
    "print(\"Summary Pipeline\")\n",
    "bert_similarity(df['summary_pipeline'], 1)\n",
    "\n",
    "print(\"Summary Summa\")\n",
    "bert_similarity(df['summary_summa'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(X):\n",
    "    model = Word2Vec(X, vector_size=100, window=5, min_count=1, workers=4)  \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['final_text'].apply(lambda x: x.split(\" \"))\n",
    "w2v_model = train_word2vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decision', 0.998510479927063),\n",
       " ('sixth', 0.9983252286911011),\n",
       " ('writ', 0.9982919096946716),\n",
       " ('circuit', 0.9981789588928223),\n",
       " ('jurisdiction', 0.9979424476623535),\n",
       " ('preliminary', 0.9979386925697327),\n",
       " ('reverse', 0.9979249835014343),\n",
       " ('grant', 0.9978001117706299),\n",
       " ('petition', 0.9977887272834778),\n",
       " ('federal', 0.9977492690086365)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(w2v_model.wv.index_to_key)\n",
    "w2v_model.wv.most_similar('supreme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d04be2ae5ec46cd6d462ae6528ee312609149e0a7486866f745c023d633a300"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
